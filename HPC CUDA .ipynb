{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YVO-OrfDDOv",
        "outputId": "05ad1ae5-9379-472b-d088-3cd0fdf81095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Collecting git+https://github.com/afnan47/cuda.git\n",
            "  Cloning https://github.com/afnan47/cuda.git to /tmp/pip-req-build-phesd2xp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/afnan47/cuda.git /tmp/pip-req-build-phesd2xp\n",
            "  Resolved https://github.com/afnan47/cuda.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4289 sha256=32c53b6f4663e89edb43a52bb67f732b3c2fcc8c2c789e2e5ef7f99280a26439\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bbo1970u/wheels/aa/f3/44/e10c1d226ec561d971fcd4b0463f6bff08602afa928a3e7bc7\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!pip install git+https://github.com/afnan47/cuda.git\n",
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cstdlib>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__ void add(int* A, int* B, int* C, int size) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < size) {\n",
        "        C[tid] = A[tid] + B[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N;\n",
        "    cout << \"Enter the size of the vectors: \";\n",
        "    cin >> N;\n",
        "\n",
        "    int* A, * B, * C;\n",
        "    size_t vectorBytes = N * sizeof(int);\n",
        "\n",
        "    A = new int[N];\n",
        "    B = new int[N];\n",
        "    C = new int[N];\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        A[i] = rand() % 10;\n",
        "        B[i] = rand() % 10;\n",
        "    }\n",
        "\n",
        "    int* X, * Y, * Z;\n",
        "    cudaMalloc(&X, vectorBytes);\n",
        "    cudaMalloc(&Y, vectorBytes);\n",
        "    cudaMalloc(&Z, vectorBytes);\n",
        "\n",
        "    cudaMemcpy(X, A, vectorBytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(Y, B, vectorBytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    add<<<1, N>>>(X, Y, Z, N);\n",
        "\n",
        "    cudaMemcpy(C, Z, vectorBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cout << \"Vector A:\";\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        cout << \" \" << A[i];\n",
        "    }\n",
        "    cout << endl;\n",
        "\n",
        "    cout << \"Vector B:\";\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        cout << \" \" << B[i];\n",
        "    }\n",
        "    cout << endl;\n",
        "\n",
        "    cout << \"Addition:\";\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        cout << \" \" << C[i];\n",
        "    }\n",
        "    cout << endl;\n",
        "\n",
        "    delete[] A;\n",
        "    delete[] B;\n",
        "    delete[] C;\n",
        "\n",
        "    cudaFree(X);\n",
        "    cudaFree(Y);\n",
        "    cudaFree(Z);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk0Sg_FxFE2P",
        "outputId": "f7aa7cf1-9d6a-4f8d-9cb1-fe612de1d175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc add.cu -o add\n",
        "!./add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxypW7WDGjjH",
        "outputId": "8e1948e5-0012-4acb-bbe4-1c82580f541d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the size of the vectors: 4\n",
            "Vector A: 3 7 3 6\n",
            "Vector B: 6 5 5 2\n",
            "Addition: 9 12 8 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_multi.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "const int N = 4;\n",
        "\n",
        "__global__ void matrixMultiply(int* A, int* B, int* C) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < N && col < N) {\n",
        "        int sum = 0;\n",
        "        for (int i = 0; i < N; ++i) {\n",
        "            sum += A[row * N + i] * B[i * N + col];\n",
        "        }\n",
        "        C[row * N + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int* A, * B, * C;\n",
        "    size_t matrixBytes = N * N * sizeof(int);\n",
        "\n",
        "    A = new int[N * N];\n",
        "    B = new int[N * N];\n",
        "    C = new int[N * N];\n",
        "\n",
        "    auto input = [&](int* matrix) {\n",
        "        cout << \"Enter elements of Matrix (\" << N << \"x\" << N << \"):\" << endl;\n",
        "        for (int i = 0; i < N * N; ++i) cin >> matrix[i];\n",
        "    };\n",
        "\n",
        "    input(A);\n",
        "    input(B);\n",
        "\n",
        "    int* X, * Y, * Z;\n",
        "    cudaMalloc(&X, matrixBytes);\n",
        "    cudaMalloc(&Y, matrixBytes);\n",
        "    cudaMalloc(&Z, matrixBytes);\n",
        "\n",
        "    cudaMemcpy(X, A, matrixBytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(Y, B, matrixBytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    matrixMultiply<<<1, dim3(N, N)>>>(X, Y, Z);\n",
        "\n",
        "    cudaMemcpy(C, Z, matrixBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cout << \"Output- Matrix size: \" << N << \"x\" << N << endl;\n",
        "    cout << \"Input Matrix 1:\" << endl;\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) cout << A[i * N + j] << \" \";\n",
        "        cout << endl;\n",
        "    }\n",
        "\n",
        "    cout << \"Input Matrix 2:\" << endl;\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) cout << B[i * N + j] << \" \";\n",
        "        cout << endl;\n",
        "    }\n",
        "\n",
        "    cout << \"Resultant matrix:\" << endl;\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) cout << C[i * N + j] << \" \";\n",
        "        cout << endl;\n",
        "    }\n",
        "\n",
        "    cout << \"Finished.\" << endl;\n",
        "\n",
        "    delete[] A;\n",
        "    delete[] B;\n",
        "    delete[] C;\n",
        "\n",
        "    cudaFree(X);\n",
        "    cudaFree(Y);\n",
        "    cudaFree(Z);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJyerXDCG1Gn",
        "outputId": "eca1a823-c54c-417b-822a-b553692d4e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix_multi.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrix_multi.cu -o matrix_multi\n",
        "!./matrix_multi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAu7f_IJH5i_",
        "outputId": "2946ccfb-cdd8-4727-9f83-46e5f8f92427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter elements of Matrix (4x4):\n",
            "3 4 6 7 \n",
            "4 7 8 1\n",
            "5 7 1 0\n",
            "5 7 9 4 \n",
            "Enter elements of Matrix (4x4):\n",
            "5 2 1 6 \n",
            "9 0 8 5 \n",
            "9 1 2 5 \n",
            "7 5 4 2\n",
            "Output- Matrix size: 4x4\n",
            "Input Matrix 1:\n",
            "3 4 6 7 \n",
            "4 7 8 1 \n",
            "5 7 1 0 \n",
            "5 7 9 4 \n",
            "Input Matrix 2:\n",
            "5 2 1 6 \n",
            "9 0 8 5 \n",
            "9 1 2 5 \n",
            "7 5 4 2 \n",
            "Resultant matrix:\n",
            "154 47 75 82 \n",
            "162 21 80 101 \n",
            "97 11 63 70 \n",
            "197 39 95 118 \n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile smma.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <climits>\n",
        "\n",
        "__global__ void min_reduction_kernel(int* arr, int size, int* result) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < size) {\n",
        "        atomicMin(result, arr[tid]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void max_reduction_kernel(int* arr, int size, int* result) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < size) {\n",
        "        atomicMax(result, arr[tid]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void sum_reduction_kernel(int* arr, int size, int* result) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < size) {\n",
        "        atomicAdd(result, arr[tid]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size;\n",
        "    std::cout << \"Enter the number of elements: \";\n",
        "    std::cin >> size;\n",
        "\n",
        "    std::vector<int> arr(size);\n",
        "    std::cout << \"Enter the elements:\" << std::endl;\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        std::cin >> arr[i];\n",
        "    }\n",
        "\n",
        "    int* d_arr;\n",
        "    int* d_result_min, * d_result_max, * d_result_sum;\n",
        "    int result_min = INT_MAX, result_max = INT_MIN, result_sum = 0;\n",
        "\n",
        "    cudaMalloc(&d_arr, size * sizeof(int));\n",
        "    cudaMemcpy(d_arr, arr.data(), size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaMalloc(&d_result_min, sizeof(int));\n",
        "    cudaMalloc(&d_result_max, sizeof(int));\n",
        "    cudaMalloc(&d_result_sum, sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_result_min, &result_min, sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_result_max, &result_max, sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_result_sum, &result_sum, sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    min_reduction_kernel<<<(size + 255) / 256, 256>>>(d_arr, size, d_result_min);\n",
        "    max_reduction_kernel<<<(size + 255) / 256, 256>>>(d_arr, size, d_result_max);\n",
        "    sum_reduction_kernel<<<(size + 255) / 256, 256>>>(d_arr, size, d_result_sum);\n",
        "\n",
        "    cudaMemcpy(&result_min, d_result_min, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&result_max, d_result_max, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&result_sum, d_result_sum, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::cout << \"Minimum value: \" << result_min << std::endl;\n",
        "    std::cout << \"Maximum value: \" << result_max << std::endl;\n",
        "    std::cout << \"Sum: \" << result_sum << std::endl;\n",
        "    std::cout << \"Average: \" << static_cast<double>(result_sum) / size << std::endl;\n",
        "\n",
        "    cudaFree(d_arr);\n",
        "    cudaFree(d_result_min);\n",
        "    cudaFree(d_result_max);\n",
        "    cudaFree(d_result_sum);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CesJEOm0dVHn",
        "outputId": "540a24dc-e952-4539-c117-064c165f55e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing smma.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc smma.cu -o smma\n",
        "!./smma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG_W20XHgF2t",
        "outputId": "b86f2a7a-017a-4e79-b1b7-aba9ae067efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of elements: 4\n",
            "Enter the elements:\n",
            "6 9 2 4\n",
            "Minimum value: 2\n",
            "Maximum value: 9\n",
            "Sum: 21\n",
            "Average: 5.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bfsdfs.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <queue>\n",
        "#include <stack>\n",
        "\n",
        "__global__ void bfs_kernel(int* adjList, int* visited, int* queue, int* queueSize, int n) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < *queueSize) {\n",
        "        int u = queue[tid];\n",
        "        if (!visited[u]) {\n",
        "            visited[u] = 1;\n",
        "            for (int i = adjList[u]; i < adjList[u + 1]; ++i) {\n",
        "                int v = adjList[i];\n",
        "                if (!visited[v]) {\n",
        "                    int idx = atomicAdd(queueSize, 1);\n",
        "                    queue[idx] = v;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void dfs_kernel(int* adjList, int* visited, int* stack, int* stackSize, int n) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < *stackSize) {\n",
        "        int u = stack[tid];\n",
        "        if (!visited[u]) {\n",
        "            visited[u] = 1;\n",
        "            for (int i = adjList[u]; i < adjList[u + 1]; ++i) {\n",
        "                int v = adjList[i];\n",
        "                if (!visited[v]) {\n",
        "                    int idx = atomicAdd(stackSize, 1);\n",
        "                    stack[idx] = v;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n, m;\n",
        "    std::cout << \"Enter the number of vertices: \";\n",
        "    std::cin >> n;\n",
        "    std::cout << \"Enter the number of edges: \";\n",
        "    std::cin >> m;\n",
        "\n",
        "    // Assuming graph is represented as an adjacency list\n",
        "    std::vector<std::vector<int>> adjList(n + 1);\n",
        "    std::cout << \"Enter the edges (format: u v):\\n\";\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        int u, v;\n",
        "        std::cin >> u >> v;\n",
        "        adjList[u].push_back(v);\n",
        "        adjList[v].push_back(u); // Assuming an undirected graph\n",
        "    }\n",
        "\n",
        "    // Allocate memory on the GPU\n",
        "    int* d_adjList, * d_visited, * d_queue, * d_queueSize, * d_stack, * d_stackSize;\n",
        "    cudaMalloc(&d_adjList, (2 * m) * sizeof(int)); // Each edge is stored twice in the adjacency list\n",
        "    cudaMalloc(&d_visited, n * sizeof(int));\n",
        "    cudaMalloc(&d_queue, n * sizeof(int));\n",
        "    cudaMalloc(&d_queueSize, sizeof(int));\n",
        "    cudaMalloc(&d_stack, n * sizeof(int));\n",
        "    cudaMalloc(&d_stackSize, sizeof(int));\n",
        "\n",
        "    // Initialize data on the GPU\n",
        "\n",
        "    // Perform BFS traversal\n",
        "    int start;\n",
        "    std::cout << \"Enter the starting vertex for BFS: \";\n",
        "    std::cin >> start;\n",
        "    cudaMemcpy(d_queue, &start, sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_queueSize, &start, sizeof(int), cudaMemcpyHostToDevice);\n",
        "    int queueSize = 1;\n",
        "    while (queueSize > 0) {\n",
        "        bfs_kernel<<<(queueSize + 255) / 256, 256>>>(d_adjList, d_visited, d_queue, d_queueSize, n);\n",
        "        cudaMemcpy(&queueSize, d_queueSize, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    }\n",
        "\n",
        "    // Perform DFS traversal\n",
        "    std::cout << \"Enter the starting vertex for DFS: \";\n",
        "    std::cin >> start;\n",
        "    cudaMemcpy(d_visited, &start, sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_stack, &start, sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_stackSize, &start, sizeof(int), cudaMemcpyHostToDevice);\n",
        "    int stackSize = 1;\n",
        "    while (stackSize > 0) {\n",
        "        dfs_kernel<<<(stackSize + 255) / 256, 256>>>(d_adjList, d_visited, d_stack, d_stackSize, n);\n",
        "        cudaMemcpy(&stackSize, d_stackSize, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    }\n",
        "\n",
        "    // Copy visited array back to host\n",
        "    int* h_visited = new int[n];\n",
        "    cudaMemcpy(h_visited, d_visited, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print BFS traversal result\n",
        "    std::cout << \"BFS traversal starting from vertex \" << start << \":\\n\";\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        if (h_visited[i]) {\n",
        "            std::cout << i << \" \";\n",
        "        }\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    // Print DFS traversal result\n",
        "    std::cout << \"DFS traversal starting from vertex \" << start << \":\\n\";\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        if (h_visited[i]) {\n",
        "            std::cout << i << \" \";\n",
        "        }\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    delete[] h_visited;\n",
        "\n",
        "    // Free memory on the GPU\n",
        "    cudaFree(d_adjList);\n",
        "    cudaFree(d_visited);\n",
        "    cudaFree(d_queue);\n",
        "    cudaFree(d_queueSize);\n",
        "    cudaFree(d_stack);\n",
        "    cudaFree(d_stackSize);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5drxN8cg3Yf",
        "outputId": "29810f28-8d9e-4a02-950f-087c1e21c145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bfsdfs.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc bfsdfs.cu -o bfsdfs\n",
        "!./bfsdfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYTuzehWjZa9",
        "outputId": "7bbc2c31-94c9-4cbc-bf43-7a3d176683df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of vertices: 5\n",
            "Enter the number of edges: 6\n",
            "Enter the edges (format: u v):\n",
            "0 1\n",
            "0 2\n",
            "1 3\n",
            "1 4\n",
            "2 4\n",
            "3 4\n",
            "Enter the starting vertex for BFS: 0\n",
            "Enter the starting vertex for DFS: 0\n",
            "BFS traversal starting from vertex 0:\n",
            "\n",
            "DFS traversal starting from vertex 0:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bubblesort.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "\n",
        "__global__ void bubbleSortParallel(int* arr, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n - 1) {\n",
        "        if (arr[idx] > arr[idx + 1]) {\n",
        "            int temp = arr[idx];\n",
        "            arr[idx] = arr[idx + 1];\n",
        "            arr[idx + 1] = temp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void bubbleSortSerial(std::vector<int>& arr) {\n",
        "    int n = arr.size();\n",
        "    bool swapped = true;\n",
        "    while (swapped) {\n",
        "        swapped = false;\n",
        "        for (int i = 0; i < n - 1; i++) {\n",
        "            if (arr[i] > arr[i + 1]) {\n",
        "                std::swap(arr[i], arr[i + 1]);\n",
        "                swapped = true;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 10000;\n",
        "    int block_size = 256;\n",
        "    int num_blocks = (n + block_size - 1) / block_size;\n",
        "\n",
        "    std::vector<int> arr(n);\n",
        "\n",
        "    // Initialize array with random values\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        arr[i] = rand() % 10000;\n",
        "    }\n",
        "\n",
        "    // Measure serial Bubble Sort performance\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "    bubbleSortSerial(arr);\n",
        "    auto stop = std::chrono::high_resolution_clock::now();\n",
        "    auto durationSerial = std::chrono::duration_cast<std::chrono::milliseconds>(stop - start);\n",
        "\n",
        "    std::cout << \"Serial Bubble Sort took \" << durationSerial.count() << \" milliseconds.\" << std::endl;\n",
        "\n",
        "    // Reset array for parallel sort\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        arr[i] = rand() % 10000;\n",
        "    }\n",
        "\n",
        "    int* d_arr;\n",
        "    cudaMalloc(&d_arr, n * sizeof(int));\n",
        "    cudaMemcpy(d_arr, arr.data(), n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Measure parallel Bubble Sort performance\n",
        "    start = std::chrono::high_resolution_clock::now();\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        bubbleSortParallel<<<num_blocks, block_size>>>(d_arr, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "    stop = std::chrono::high_resolution_clock::now();\n",
        "    auto durationParallel = std::chrono::duration_cast<std::chrono::milliseconds>(stop - start);\n",
        "\n",
        "    std::cout << \"Parallel Bubble Sort took \" << durationParallel.count() << \" milliseconds.\" << std::endl;\n",
        "\n",
        "    cudaMemcpy(arr.data(), d_arr, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_arr);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "2NtJcj6fjbrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcdbf02f-9704-423a-be4b-9c8423d6639a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bubblesort.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc bubblesort.cu -o bubblesort\n",
        "!./bubblesort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAAZEUlFFnmi",
        "outputId": "fb3ae298-58f0-48f4-b301-9400ed8d8e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial Bubble Sort took 1125 milliseconds.\n",
            "Parallel Bubble Sort took 134 milliseconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mergesort.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Serial merge sort implementation\n",
        "void merge(int* arr, int l, int m, int r) {\n",
        "    int n1 = m - l + 1;\n",
        "    int n2 = r - m;\n",
        "\n",
        "    // Create temporary arrays\n",
        "    int L[n1], R[n2];\n",
        "\n",
        "    // Copy data to temporary arrays L[] and R[]\n",
        "    for (int i = 0; i < n1; i++)\n",
        "        L[i] = arr[l + i];\n",
        "    for (int j = 0; j < n2; j++)\n",
        "        R[j] = arr[m + 1 + j];\n",
        "\n",
        "    // Merge the temporary arrays back into arr[l..r]\n",
        "    int i = 0; // Initial index of first subarray\n",
        "    int j = 0; // Initial index of second subarray\n",
        "    int k = l; // Initial index of merged subarray\n",
        "    while (i < n1 && j < n2) {\n",
        "        if (L[i] <= R[j]) {\n",
        "            arr[k] = L[i];\n",
        "            i++;\n",
        "        } else {\n",
        "            arr[k] = R[j];\n",
        "            j++;\n",
        "        }\n",
        "        k++;\n",
        "    }\n",
        "\n",
        "    // Copy the remaining elements of L[], if any\n",
        "    while (i < n1) {\n",
        "        arr[k] = L[i];\n",
        "        i++;\n",
        "        k++;\n",
        "    }\n",
        "\n",
        "    // Copy the remaining elements of R[], if any\n",
        "    while (j < n2) {\n",
        "        arr[k] = R[j];\n",
        "        j++;\n",
        "        k++;\n",
        "    }\n",
        "}\n",
        "\n",
        "void mergeSort(int* arr, int l, int r) {\n",
        "    if (l < r) {\n",
        "        int m = l + (r - l) / 2;\n",
        "        mergeSort(arr, l, m);\n",
        "        mergeSort(arr, m + 1, r);\n",
        "        merge(arr, l, m, r);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Parallel merge sort kernel\n",
        "__global__ void mergeSortKernel(int* arr, int* temp, int size, int width) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int start = idx * width * 2;\n",
        "\n",
        "    if (start < size) {\n",
        "        int end = min(start + width * 2, size);\n",
        "        int mid = start + width;\n",
        "        int i = start;\n",
        "        int j = mid;\n",
        "        int k = start;\n",
        "\n",
        "        while (i < mid && j < end) {\n",
        "            if (arr[i] <= arr[j]) {\n",
        "                temp[k] = arr[i];\n",
        "                i++;\n",
        "            } else {\n",
        "                temp[k] = arr[j];\n",
        "                j++;\n",
        "            }\n",
        "            k++;\n",
        "        }\n",
        "\n",
        "        while (i < mid) {\n",
        "            temp[k] = arr[i];\n",
        "            i++;\n",
        "            k++;\n",
        "        }\n",
        "\n",
        "        while (j < end) {\n",
        "            temp[k] = arr[j];\n",
        "            j++;\n",
        "            k++;\n",
        "        }\n",
        "\n",
        "        // Copy merged values back to original array\n",
        "        for (int i = start; i < end; i++) {\n",
        "            arr[i] = temp[i];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void mergeSortParallel(int* arr, int size) {\n",
        "    int* temp;\n",
        "    cudaMalloc(&temp, size * sizeof(int));\n",
        "    cudaMemcpy(temp, arr, size * sizeof(int), cudaMemcpyDeviceToDevice);\n",
        "\n",
        "    for (int width = 1; width < size; width *= 2) {\n",
        "        int num_blocks = (size + (2 * width) - 1) / (2 * width);\n",
        "        mergeSortKernel<<<num_blocks, BLOCK_SIZE>>>(arr, temp, size, width);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    cudaFree(temp);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 10000;\n",
        "    std::vector<int> arr_serial(n);\n",
        "    std::vector<int> arr_parallel(n);\n",
        "\n",
        "    // Initialize arrays with random values\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        arr_serial[i] = rand() % 1000; // Generating random numbers between 0 and 999\n",
        "        arr_parallel[i] = arr_serial[i];\n",
        "    }\n",
        "\n",
        "    // Serial merge sort\n",
        "    auto start_serial = std::chrono::high_resolution_clock::now();\n",
        "    mergeSort(arr_serial.data(), 0, n - 1);\n",
        "    auto end_serial = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Parallel merge sort\n",
        "    auto start_parallel = std::chrono::high_resolution_clock::now();\n",
        "    mergeSortParallel(arr_parallel.data(), n);\n",
        "    cudaDeviceSynchronize();\n",
        "    auto end_parallel = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Print timing information\n",
        "    std::chrono::duration<double, std::milli> duration_serial = end_serial - start_serial;\n",
        "    std::cout << \"Serial Merge Sort took \" << duration_serial.count() << \" milliseconds.\" << std::endl;\n",
        "\n",
        "    std::chrono::duration<double, std::milli> duration_parallel = end_parallel - start_parallel;\n",
        "    std::cout << \"Parallel Merge Sort took \" << duration_parallel.count() << \" milliseconds.\" << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hZBMf1qXFqRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef990088-b837-49e3-fdde-efbdcb29d98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mergesort.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc mergesort.cu -o mergesort\n",
        "!./mergesort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzB5yBqLfFGM",
        "outputId": "e3ed6159-a3ff-4fcd-b8bb-4b891cec335c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial Merge Sort took 3.23756 milliseconds.\n",
            "Parallel Merge Sort took 404.099 milliseconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mergesort1.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Serial merge sort implementation\n",
        "void merge(int* arr, int l, int m, int r) {\n",
        "    int n1 = m - l + 1;\n",
        "    int n2 = r - m;\n",
        "\n",
        "    // Create temporary arrays\n",
        "    int L[n1], R[n2];\n",
        "\n",
        "    // Copy data to temporary arrays L[] and R[]\n",
        "    for (int i = 0; i < n1; i++)\n",
        "        L[i] = arr[l + i];\n",
        "    for (int j = 0; j < n2; j++)\n",
        "        R[j] = arr[m + 1 + j];\n",
        "\n",
        "    // Merge the temporary arrays back into arr[l..r]\n",
        "    int i = 0; // Initial index of first subarray\n",
        "    int j = 0; // Initial index of second subarray\n",
        "    int k = l; // Initial index of merged subarray\n",
        "    while (i < n1 && j < n2) {\n",
        "        if (L[i] <= R[j]) {\n",
        "            arr[k] = L[i];\n",
        "            i++;\n",
        "        } else {\n",
        "            arr[k] = R[j];\n",
        "            j++;\n",
        "        }\n",
        "        k++;\n",
        "    }\n",
        "\n",
        "    // Copy the remaining elements of L[], if any\n",
        "    while (i < n1) {\n",
        "        arr[k] = L[i];\n",
        "        i++;\n",
        "        k++;\n",
        "    }\n",
        "\n",
        "    // Copy the remaining elements of R[], if any\n",
        "    while (j < n2) {\n",
        "        arr[k] = R[j];\n",
        "        j++;\n",
        "        k++;\n",
        "    }\n",
        "}\n",
        "\n",
        "void mergeSort(int* arr, int l, int r) {\n",
        "    if (l < r) {\n",
        "        int m = l + (r - l) / 2;\n",
        "        mergeSort(arr, l, m);\n",
        "        mergeSort(arr, m + 1, r);\n",
        "        merge(arr, l, m, r);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Parallel merge sort kernel\n",
        "__global__ void mergeSortKernel(int* arr, int* temp, int size, int width) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int start = idx * width * 2;\n",
        "\n",
        "    if (start < size) {\n",
        "        int end = min(start + width * 2, size);\n",
        "        int mid = start + width;\n",
        "        int i = start;\n",
        "        int j = mid;\n",
        "        int k = start;\n",
        "\n",
        "        while (i < mid && j < end) {\n",
        "            if (arr[i] <= arr[j]) {\n",
        "                temp[k] = arr[i];\n",
        "                i++;\n",
        "            } else {\n",
        "                temp[k] = arr[j];\n",
        "                j++;\n",
        "            }\n",
        "            k++;\n",
        "        }\n",
        "\n",
        "        while (i < mid) {\n",
        "            temp[k] = arr[i];\n",
        "            i++;\n",
        "            k++;\n",
        "        }\n",
        "\n",
        "        while (j < end) {\n",
        "            temp[k] = arr[j];\n",
        "            j++;\n",
        "            k++;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void mergeSortParallel(int* arr, int size) {\n",
        "    int* temp;\n",
        "    cudaMalloc(&temp, size * sizeof(int));\n",
        "    cudaMemcpy(temp, arr, size * sizeof(int), cudaMemcpyDeviceToDevice);\n",
        "\n",
        "    for (int width = 1; width < size; width *= 2) {\n",
        "        int num_blocks = (size + (2 * width) - 1) / (2 * width);\n",
        "        mergeSortKernel<<<num_blocks, BLOCK_SIZE>>>(arr, temp, size, width);\n",
        "        cudaDeviceSynchronize();\n",
        "        cudaMemcpy(arr, temp, size * sizeof(int), cudaMemcpyDeviceToDevice); // Copy merged array back\n",
        "    }\n",
        "\n",
        "    cudaFree(temp);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 1000;\n",
        "    std::vector<int> arr_serial(n);\n",
        "    std::vector<int> arr_parallel(n);\n",
        "\n",
        "    // Initialize arrays with random values\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        arr_serial[i] = rand() % 1000; // Generating random numbers between 0 and 999\n",
        "        arr_parallel[i] = arr_serial[i];\n",
        "    }\n",
        "\n",
        "    // Serial merge sort\n",
        "    auto start_serial = std::chrono::high_resolution_clock::now();\n",
        "    mergeSort(arr_serial.data(), 0, n - 1);\n",
        "    auto end_serial = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Parallel merge sort\n",
        "    auto start_parallel = std::chrono::high_resolution_clock::now();\n",
        "    mergeSortParallel(arr_parallel.data(), n);\n",
        "    cudaDeviceSynchronize();\n",
        "    auto end_parallel = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Print timing information\n",
        "    std::chrono::duration<double, std::milli> duration_serial = end_serial - start_serial;\n",
        "    std::cout << \"Serial Merge Sort took \" << duration_serial.count() << \" milliseconds.\" << std::endl;\n",
        "\n",
        "    std::chrono::duration<double, std::milli> duration_parallel = end_parallel - start_parallel;\n",
        "    std::cout << \"Parallel Merge Sort took \" << duration_parallel.count() << \" milliseconds.\" << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCw0lALQh43x",
        "outputId": "0dac24e8-990d-4324-e3cd-8682f5f069e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mergesort1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc mergesort1.cu -o mergesort1\n",
        "!./mergesort1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bipecQtpFJb",
        "outputId": "e70157bc-2d0c-41a0-b7ca-2867ce751916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial Merge Sort took 0.138305 milliseconds.\n",
            "Parallel Merge Sort took 257.978 milliseconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mergenew.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <thrust/device_vector.h>\n",
        "#include <thrust/host_vector.h>\n",
        "#include <thrust/merge.h>\n",
        "#include <thrust/sort.h>\n",
        "#include <chrono>\n",
        "\n",
        "// Merge kernel to perform merging of two sorted arrays on the GPU\n",
        "__global__ void merge(int* input1, int* input2, int* output, int size1, int size2) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < size1 + size2) {\n",
        "        if (idx < size1) {\n",
        "            output[idx] = input1[idx];\n",
        "        } else {\n",
        "            output[idx] = input2[idx - size1];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Recursive function to perform merge sort using CUDA Thrust\n",
        "void mergeSort(thrust::device_vector<int>& data) {\n",
        "    if (data.size() <= 1) return;\n",
        "\n",
        "    int mid = data.size() / 2;\n",
        "\n",
        "    // Split the data into two halves\n",
        "    thrust::device_vector<int> left(data.begin(), data.begin() + mid);\n",
        "    thrust::device_vector<int> right(data.begin() + mid, data.end());\n",
        "\n",
        "    // Recursively sort each half\n",
        "    mergeSort(left);\n",
        "    mergeSort(right);\n",
        "\n",
        "    // Merge the sorted halves\n",
        "    thrust::device_vector<int> merged(data.size());\n",
        "    merge<<<(data.size() + 255) / 256, 256>>>(thrust::raw_pointer_cast(left.data()),\n",
        "                                               thrust::raw_pointer_cast(right.data()),\n",
        "                                               thrust::raw_pointer_cast(merged.data()),\n",
        "                                               left.size(), right.size());\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy the merged data back to the original vector\n",
        "    data = merged;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Generate random input data\n",
        "    int n = 10000;\n",
        "    thrust::host_vector<int> h_data(n);\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        h_data[i] = rand() % 10000;\n",
        "    }\n",
        "\n",
        "    // Transfer data to the device\n",
        "    thrust::device_vector<int> d_data = h_data;\n",
        "\n",
        "    // Measure serial merge sort performance\n",
        "    auto start_serial = std::chrono::high_resolution_clock::now();\n",
        "    thrust::sort(h_data.begin(), h_data.end());\n",
        "    auto end_serial = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::cout << \"Serial Merge Sort took \" << std::chrono::duration_cast<std::chrono::milliseconds>(end_serial - start_serial).count() << \" milliseconds.\" << std::endl;\n",
        "\n",
        "    // Measure parallel merge sort performance\n",
        "    auto start_parallel = std::chrono::high_resolution_clock::now();\n",
        "    mergeSort(d_data);\n",
        "    auto end_parallel = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::cout << \"Parallel Merge Sort took \" << std::chrono::duration_cast<std::chrono::milliseconds>(end_parallel - start_parallel).count() << \" milliseconds.\" << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_JYW_8zpNXK",
        "outputId": "d3771690-e117-473a-86bf-68156d16974d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mergenew.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc mergenew.cu -o mergenew\n",
        "!./mergenew"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tm8lG66qalA",
        "outputId": "2a97d667-7452-490d-a5a6-09f02622a8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial Merge Sort took 11 milliseconds.\n",
            "Parallel Merge Sort took 829 milliseconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdwSV5d3qih6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
